import os
import json
import django

# Ø³Øª Ú©Ø±Ø¯Ù† ØªÙ†Ø¸ÛŒÙ…Ø§Øª Django
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "api.settings")
django.setup()

from tests.models import Question, Folder


def split_topics(topics):
    """ØªØ¬Ø²ÛŒÙ‡ topics Ø¨Ù‡ source, publish_date, folders"""
    source_parts, folder_parts = [], []
    publish_date = None

    for t in topics:
        if t.isdigit() and len(t) == 4:  # Ø³Ø§Ù„
            publish_date = t
        elif any(kw in t for kw in ["Ú©Ù†Ú©ÙˆØ±", "Ø®Ø§Ø±Ø¬", "Ù†ÙˆØ¨Øª", "Ø¹Ù„ÙˆÛŒ", "Ø¢Ø²Ù…ÙˆÙ†"]):
            source_parts.append(t)
        else:
            folder_parts.append(t)

    source = " / ".join(source_parts) if source_parts else None
    return source, publish_date, folder_parts


def normalize_text(text):
    """Ù…ØªÙ† Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù†Ø±Ù…Ø§Ù„ Ú©Ù†Ø¯"""
    import re
    
    # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
    text = re.sub(r'\s+', ' ', text.strip())
    
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ LaTeX commands
    # ØªØ¨Ø¯ÛŒÙ„ \\ Ø¨Ù‡ \ (Ø¨Ø±Ø§ÛŒ LaTeX line breaks)
    text = re.sub(r'\\\\+', r'\\', text)
    
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙØ§ØµÙ„Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¯Ø± LaTeX
    text = re.sub(r'\\\s+', r'\\', text)  # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² Ø¨Ú©â€ŒØ§Ø³Ù„Ø´
    text = re.sub(r'\s+\\', r'\\', text)  # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡ Ù‚Ø¨Ù„ Ø§Ø² Ø¨Ú©â€ŒØ§Ø³Ù„Ø´
    
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ matrix Ùˆ aligned environments
    text = re.sub(r'\\begin\s*\{\s*matrix\s*\}', r'\\begin{matrix}', text)
    text = re.sub(r'\\end\s*\{\s*matrix\s*\}', r'\\end{matrix}', text)
    text = re.sub(r'\\begin\s*\{\s*aligned\s*\}', r'\\begin{aligned}', text)
    text = re.sub(r'\\end\s*\{\s*aligned\s*\}', r'\\end{aligned}', text)
    
    # Ø­Ø°Ù LaTeX tags Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ù‡ØªØ± (Ø§Ø®ØªÛŒØ§Ø±ÛŒ - ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª)
    text_for_comparison = re.sub(r'\$[^$]*\$', ' [MATH] ', text)
    text_for_comparison = re.sub(r'\\[a-zA-Z]+\{[^}]*\}', ' [LATEX] ', text_for_comparison)
    text_for_comparison = re.sub(r'\\[a-zA-Z]+', ' [CMD] ', text_for_comparison)
    
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ú©Ù„Ù…Ø§Øª ÙØ§Ø±Ø³ÛŒ Ù…ØªØµÙ„/Ø¬Ø¯Ø§
    # "Ø¨Ù‡ØµÙˆØ±Øª" Ùˆ "Ø¨Ù‡ ØµÙˆØ±Øª"
    text = re.sub(r'Ø¨Ù‡\s*ØµÙˆØ±Øª', 'Ø¨Ù‡ØµÙˆØ±Øª', text)
    text = re.sub(r'Ø¨ØµÙˆØ±Øª', 'Ø¨Ù‡ØµÙˆØ±Øª', text)
    
    # "Ø¯Ø±ØµÙˆØ±Øª" Ùˆ "Ø¯Ø± ØµÙˆØ±Øª"
    text = re.sub(r'Ø¯Ø±\s*ØµÙˆØ±Øª', 'Ø¯Ø±ØµÙˆØ±Øª', text)
    
    # "Ø¨Ù‡Ø¹Ù†ÙˆØ§Ù†" Ùˆ "Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù†"
    text = re.sub(r'Ø¨Ù‡\s*Ø¹Ù†ÙˆØ§Ù†', 'Ø¨Ù‡Ø¹Ù†ÙˆØ§Ù†', text)
    
    # "Ø¯Ø±Ù†Ø¸Ø±" Ùˆ "Ø¯Ø± Ù†Ø¸Ø±"
    text = re.sub(r'Ø¯Ø±\s*Ù†Ø¸Ø±', 'Ø¯Ø±Ù†Ø¸Ø±', text)
    
    # Ø­Ø°Ù Ø¹Ù„Ø§Ù…Ø§Øª Ù†Ú¯Ø§Ø±Ø´ÛŒ Ù…ØªØ¯Ø§ÙˆÙ„
    punctuation_chars = 'ØŒØ›:ØŸ!.;:?!()[]{}Â«Â»""\'`'
    for char in punctuation_chars:
        text = text.replace(char, '')
    
    # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ù…Ø¬Ø¯Ø¯ Ø¨Ø¹Ø¯ Ø§Ø² Ø­Ø°Ù Ø¹Ù„Ø§Ù…Ø§Øª
    text = re.sub(r'\s+', ' ', text.strip())
    
    # ØªØ¨Ø¯ÛŒÙ„ ÛŒ/ÙŠ Ùˆ Ú©/Ùƒ Ø¨Ù‡ Ø­Ø§Ù„Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
    text = text.replace('ÙŠ', 'ÛŒ').replace('Ùƒ', 'Ú©')
    
    # Ø­Ø°Ù Ø§Ø¹Ø±Ø§Ø¨ ÙØ§Ø±Ø³ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
    arabic_diacritics = 'ÙÙÙÙ‘Ù’Ù°Ù±Ù²Ù³Ù´ÙµÙ¶Ù·Ù¸Ù¹ÙºÙ»Ù¼Ù½Ù¾Ù¿Ú€ÚÚ‚ÚƒÚ„Ú…Ú†Ú‡ÚˆÚ‰ÚŠÚ‹ÚŒÚÚÚÚÚ‘Ú’Ú“Ú”Ú•Ú–Ú—Ú˜Ú™ÚšÚ›ÚœÚÚÚŸÚ Ú¡Ú¢Ú£Ú¤Ú¥Ú¦Ú§Ú¨Ú©ÚªÚ«Ú¬Ú­Ú®Ú¯Ú°Ú±Ú²Ú³Ú´ÚµÚ¶Ú·Ú¸Ú¹ÚºÚ»Ú¼Ú½Ú¾Ú¿'
    for char in arabic_diacritics:
        text = text.replace(char, '')
    
    return text.lower()


def test_normalization(text1, text2):
    """ØªØ³Øª Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ùˆ Ù…ØªÙ† Ùˆ Ù†Ù…Ø§ÛŒØ´ ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§"""
    print("=== ØªØ³Øª Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ===")
    print(f"Ù…ØªÙ† Ø§ÙˆÙ„ (Ø§ØµÙ„ÛŒ): {text1[:100]}...")
    print(f"Ù…ØªÙ† Ø¯ÙˆÙ… (Ø§ØµÙ„ÛŒ): {text2[:100]}...")
    print()
    
    norm1 = normalize_text(text1)
    norm2 = normalize_text(text2)
    
    print(f"Ù…ØªÙ† Ø§ÙˆÙ„ (Ù†Ø±Ù…Ø§Ù„): {norm1[:100]}...")
    print(f"Ù…ØªÙ† Ø¯ÙˆÙ… (Ù†Ø±Ù…Ø§Ù„): {norm2[:100]}...")
    print()
    
    similarity = calculate_similarity(norm1, norm2)
    print(f"Ø´Ø¨Ø§Ù‡Øª: {similarity:.2f}%")
    
    if norm1 == norm2:
        print("âœ… ØªØ·Ø§Ø¨Ù‚ Ú©Ø§Ù…Ù„!")
    else:
        print("âŒ ØªØ·Ø§Ø¨Ù‚ Ú©Ø§Ù…Ù„ Ù†ÛŒØ³Øª")
        
        # Ù†Ù…Ø§ÛŒØ´ ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§
        from difflib import unified_diff
        diff = list(unified_diff(
            norm1.splitlines(keepends=True),
            norm2.splitlines(keepends=True),
            fromfile='JSON',
            tofile='DB',
            lineterm=''
        ))
        if diff:
            print("\nØªÙØ§ÙˆØªâ€ŒÙ‡Ø§:")
            for line in diff[:10]:  # ÙÙ‚Ø· 10 Ø®Ø· Ø§ÙˆÙ„
                print(line.rstrip())
    
    print("=" * 50)


def calculate_similarity(text1, text2):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¯Ùˆ Ù…ØªÙ† (0 ØªØ§ 100)"""
    from difflib import SequenceMatcher
    return SequenceMatcher(None, text1, text2).ratio() * 100


def find_question_by_text(question_text, similarity_threshold=85):
    """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø³ÙˆØ§Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙ† Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª ØªØ·Ø¨ÛŒÙ‚ ØªÙ‚Ø±ÛŒØ¨ÛŒ"""
    normalized_input = normalize_text(question_text)
    
    # Ù…Ø±Ø­Ù„Ù‡ 1: Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¯Ù‚ÛŒÙ‚
    for question in Question.objects.all():
        normalized_db = normalize_text(question.question_text)
        if normalized_db == normalized_input:
            return question, 100  # ØªØ·Ø§Ø¨Ù‚ Ú©Ø§Ù…Ù„
    
    # Ù…Ø±Ø­Ù„Ù‡ 2: Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ Ø¨Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª
    candidates = []
    
    for question in Question.objects.all():
        normalized_db = normalize_text(question.question_text)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª
        similarity = calculate_similarity(normalized_input, normalized_db)
        
        if similarity >= similarity_threshold:
            candidates.append((question, similarity))
    
    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø´Ø¨Ø§Ù‡Øª
    if candidates:
        candidates.sort(key=lambda x: x[1], reverse=True)
        best_match = candidates[0]
        print(f"ğŸ” ØªØ·Ø§Ø¨Ù‚ ØªÙ‚Ø±ÛŒØ¨ÛŒ Ù¾ÛŒØ¯Ø§ Ø´Ø¯ Ø¨Ø§ {best_match[1]:.1f}% Ø´Ø¨Ø§Ù‡Øª")
        return best_match[0], best_match[1]
    
    # Ù…Ø±Ø­Ù„Ù‡ 3: Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ (Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ§Ø±Ø¯ Ø´Ø¯ÛŒØ¯Ø§Ù‹ ØªØºÛŒÛŒØ± ÛŒØ§ÙØªÙ‡)
    input_words = set(normalized_input.split())
    if len(input_words) < 3:  # Ø§Ú¯Ø± Ù…ØªÙ† Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ø¨Ø§Ø´Ø¯ Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ø±Ø§ Ø±Ø¯ Ú©Ù†
        return None, 0
    
    for question in Question.objects.all():
        normalized_db = normalize_text(question.question_text)
        db_words = set(normalized_db.split())
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ø±ØµØ¯ Ú©Ù„Ù…Ø§Øª Ù…Ø´ØªØ±Ú©
        if len(db_words) == 0:
            continue
            
        common_words = input_words.intersection(db_words)
        similarity = (len(common_words) / max(len(input_words), len(db_words))) * 100
        
        if similarity >= 70:  # Ø­Ø¯Ø§Ù‚Ù„ 70% Ú©Ù„Ù…Ø§Øª Ù…Ø´ØªØ±Ú©
            print(f"ğŸ” ØªØ·Ø§Ø¨Ù‚ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù¾ÛŒØ¯Ø§ Ø´Ø¯ Ø¨Ø§ {similarity:.1f}% Ú©Ù„Ù…Ø§Øª Ù…Ø´ØªØ±Ú©")
            return question, similarity
    
    return None, 0


def create_folder_hierarchy(folder_topics):
    """Ø§ÛŒØ¬Ø§Ø¯ Ø³Ù„Ø³Ù„Ù‡ Ù…Ø±Ø§ØªØ¨ÛŒ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ùˆ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù„ÛŒØ³Øª Ù‡Ù…Ù‡ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§"""
    parent = None
    created_folders = []
    
    for folder_name in folder_topics:
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù¾ÙˆØ´Ù‡ Ø¨Ø§ Ù†Ø§Ù… Ùˆ parent Ù…Ø´Ø®Øµ
        if parent:
            folder = Folder.objects.filter(name=folder_name, parent=parent).first()
        else:
            folder = Folder.objects.filter(name=folder_name, parent__isnull=True).first()
        
        if not folder:
            folder = Folder.objects.create(name=folder_name, parent=parent)
            print(f"ğŸ“ Ù¾ÙˆØ´Ù‡ Ø¬Ø¯ÛŒØ¯ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: {folder_name}")
        
        created_folders.append(folder)
        parent = folder
    
    return created_folders


def process_json_file(file_path):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© ÙØ§ÛŒÙ„ JSON"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            raw_data = json.load(f)
        
        # ØµØ§Ù Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        all_data = []
        for block in raw_data:
            if isinstance(block, list):
                all_data.extend(block)
            else:
                all_data.append(block)
        
        return all_data
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ {file_path}: {str(e)}")
        return []


def get_json_files(path):
    """Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ JSON Ø§Ø² Ù…Ø³ÛŒØ± Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡"""
    json_files = []
    
    if os.path.isfile(path):
        # Ø§Ú¯Ø± ÛŒÚ© ÙØ§ÛŒÙ„ Ø§Ø³Øª
        if path.lower().endswith('.json'):
            json_files.append(path)
        else:
            print(f"âŒ ÙØ§ÛŒÙ„ {path} ÛŒÚ© ÙØ§ÛŒÙ„ JSON Ù†ÛŒØ³Øª!")
    elif os.path.isdir(path):
        # Ø§Ú¯Ø± ÛŒÚ© Ù¾ÙˆØ´Ù‡ Ø§Ø³Øª
        for root, dirs, files in os.walk(path):
            for file in files:
                if file.lower().endswith('.json'):
                    json_files.append(os.path.join(root, file))
    else:
        print(f"âŒ Ù…Ø³ÛŒØ± {path} ÛŒØ§ÙØª Ù†Ø´Ø¯!")
    
    return json_files


def update_question_folders(path, similarity_threshold=85, debug_mode=False):
    """Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙÙˆÙ„Ø¯Ø±Ù‡Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø§Ø² ÙØ§ÛŒÙ„ ÛŒØ§ Ù¾ÙˆØ´Ù‡ JSON"""
    
    if not os.path.exists(path):
        print(f"âŒ Ù…Ø³ÛŒØ± {path} ÛŒØ§ÙØª Ù†Ø´Ø¯!")
        return
    
    print(f"ğŸ¯ Ø­Ø¯ Ø¢Ø³ØªØ§Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª: {similarity_threshold}%")
    if debug_mode:
        print("ğŸ› Ø­Ø§Ù„Øª Ø¯ÛŒØ¨Ø§Ú¯ ÙØ¹Ø§Ù„ Ø§Ø³Øª")
    print("   (Ø¨Ø±Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ…: python script.py path --threshold 80)")
    print()
    
    # Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ JSON
    json_files = get_json_files(path)
    
    if not json_files:
        print(f"âŒ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ JSON Ø¯Ø± Ù…Ø³ÛŒØ± {path} ÛŒØ§ÙØª Ù†Ø´Ø¯!")
        return
    
    print(f"ğŸ“ Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡: {len(json_files)} ÙØ§ÛŒÙ„ JSON")
    for file in json_files:
        print(f"   â€¢ {os.path.basename(file)}")
    print()
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
    all_questions = []
    for json_file in json_files:
        print(f"ğŸ“„ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´: {os.path.basename(json_file)}")
        file_data = process_json_file(json_file)
        all_questions.extend(file_data)
        print(f"   âœ… {len(file_data)} Ø³ÙˆØ§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
    
    print(f"\nğŸ” Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ {len(all_questions)} Ø³ÙˆØ§Ù„ Ø§Ø² {len(json_files)} ÙØ§ÛŒÙ„...")
    
    updated_count = 0
    not_found_count = 0
    processed_files = {}
    
    for i, item in enumerate(all_questions, 1):
        question_text = item.get("question", "").strip()
        if not question_text:
            print(f"âš ï¸  Ø³ÙˆØ§Ù„ {i}: Ù…ØªÙ† Ø³ÙˆØ§Ù„ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª")
            continue
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø³ÙˆØ§Ù„ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        result = find_question_by_text(question_text, similarity_threshold)
        if result[0] is None:  # Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯
            print(f"âŒ Ø³ÙˆØ§Ù„ {i}: Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ - {question_text[:50]}...")
            if debug_mode:
                print(f"   Ù…ØªÙ† Ú©Ø§Ù…Ù„: {question_text}")
                # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ù…ÙˆØ§Ø±Ø¯
                print("   ğŸ” Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ù…ÙˆØ§Ø±Ø¯:")
                candidates = []
                for q in Question.objects.all()[:10]:  # ÙÙ‚Ø· 10 Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ Ø¨Ø±Ø§ÛŒ ØªØ³Øª
                    similarity = calculate_similarity(normalize_text(question_text), normalize_text(q.question_text))
                    if similarity > 50:
                        candidates.append((q, similarity))
                candidates.sort(key=lambda x: x[1], reverse=True)
                for q, sim in candidates[:3]:
                    print(f"     - {sim:.1f}%: {q.question_text[:60]}...")
            not_found_count += 1
            continue
        
        question, similarity = result
        if similarity < 100:
            print(f"ğŸ“ Ø³ÙˆØ§Ù„ {i}: ØªØ·Ø§Ø¨Ù‚ ØªÙ‚Ø±ÛŒØ¨ÛŒ Ø¨Ø§ {similarity:.1f}% Ø´Ø¨Ø§Ù‡Øª")
            if debug_mode:
                print(f"   JSON: {question_text[:80]}...")
                print(f"   DB:   {question.question_text[:80]}...")
                test_normalization(question_text, question.question_text)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙˆÙ„Ø¯Ø±Ù‡Ø§ Ø§Ø² topics
        source, publish_date, folder_topics = split_topics(item.get("topics", []))
        
        if not folder_topics:
            print(f"âš ï¸  Ø³ÙˆØ§Ù„ {i}: Ù‡ÛŒÚ† ÙÙˆÙ„Ø¯Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÛŒØ§ÙØª Ù†Ø´Ø¯")
            continue
        
        # Ø§ÛŒØ¬Ø§Ø¯ ÙÙˆÙ„Ø¯Ø±Ù‡Ø§
        folders = create_folder_hierarchy(folder_topics)
        
        # Ø­Ø°Ù ÙÙˆÙ„Ø¯Ø±Ù‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ùˆ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙÙˆÙ„Ø¯Ø±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
        question.folders.clear()
        for folder in folders:
            question.folders.add(folder)
        
        # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø³Ø§ÛŒØ± ÙÛŒÙ„Ø¯Ù‡Ø§ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²
        if source and source != question.source:
            question.source = source
        if publish_date and publish_date != question.publish_date:
            question.publish_date = publish_date
        
        question.save()
        
        folder_names = [f.name for f in folders]
        print(f"âœ… Ø³ÙˆØ§Ù„ {i}: Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯ - {question.public_id}")
        print(f"   ÙÙˆÙ„Ø¯Ø±Ù‡Ø§: {' > '.join(folder_names)}")
        
        updated_count += 1
    
    print("\n" + "="*60)
    print(f"ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ:")
    print(f"   â€¢ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡: {len(json_files)}")
    print(f"   â€¢ Ú©Ù„ Ø³ÙˆØ§Ù„Ø§Øª: {len(all_questions)}")
    print(f"   â€¢ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯Ù‡: {updated_count}")
    print(f"   â€¢ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯Ù‡: {not_found_count}")
    print("="*60)


if __name__ == "__main__":
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(description="Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙÙˆÙ„Ø¯Ø±Ù‡Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø§Ø² ÙØ§ÛŒÙ„ ÛŒØ§ Ù¾ÙˆØ´Ù‡ JSON")
    parser.add_argument("path", help="Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ JSON ÛŒØ§ Ù¾ÙˆØ´Ù‡ Ø­Ø§ÙˆÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ JSON")
    parser.add_argument("--threshold", "-t", type=int, default=85, 
                       help="Ø­Ø¯ Ø¢Ø³ØªØ§Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø±Ø§ÛŒ ØªØ·Ø§Ø¨Ù‚ ØªÙ‚Ø±ÛŒØ¨ÛŒ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 85)")
    parser.add_argument("--debug", "-d", action="store_true",
                       help="ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§Ù„Øª Ø¯ÛŒØ¨Ø§Ú¯ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨ÛŒØ´ØªØ±")
    
    # Parse arguments but also support old style for backwards compatibility
    if len(sys.argv) == 2 and not sys.argv[1].startswith('-'):
        # Old style: just path
        path = sys.argv[1]
        threshold = 85
        debug_mode = False
    elif len(sys.argv) >= 2:
        args = parser.parse_args()
        path = args.path
        threshold = args.threshold
        debug_mode = args.debug
    else:
        parser.print_help()
        print("\nÙ…Ø«Ø§Ù„â€ŒÙ‡Ø§:")
        print("python update_question_folders.py data/50.json")
        print("python update_question_folders.py data/questions/ --threshold 80")
        print("python update_question_folders.py \"C:\\Users\\Username\\Downloads\\questions\\\" -t 90 --debug")
        sys.exit(1)
    
    update_question_folders(path, threshold, debug_mode)